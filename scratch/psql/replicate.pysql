
-- implement Logical Replication in pre-9.4 postgres
--  also, this implementation is tuned to solve the specific problem of replicating to the web 
-- while 9.4 has Logical Replication, it a) conflicts with b) demands you poll (whereas CouchDB, whose filtered replication feature is what I'm trying to implement, [supports](http://couchdb.readthedocs.org/en/latest/replication/protocol.html#filter-replication) both pull ((long-)polling mode (the details of the long-polling variant are irrelevant to us; it's simply a workaround for HTTP being stupid, which we sidestep by not using HTTP at all)) and push ("streaming mode")

-- TODO:
--  [ ] Install these hooks into a spare tablespace
--  [ ] Make sure these hooks can be deployed to production servers
--  [ ] Do load tests to make sure these hooks don't lag out the server; actually formatting and compacting the IP packets should perhaps be pushed out to an external process

CREATE OR REPLACE LANGUAGE plpython2u;

-- DESIGN: abuse tables and triggers to construct event queues 
-- 
--  replicate_register(_table) sets watch_table() as a trigger on the given _table, if it is not already there
--   watch_table() formats changes as HRDJ (hobo-rowdelta json) format and writes to my_pg_replicate
--   my_pg_replicate is used as a queue; broadcast() is set as an INSTEAD OF trigger on it, stopping any data actually getting written to it; 
--   instead, broadcast() takes the HRDJ it received indirectly from watch() and copies it to all active clients; 
--  
--  on the "client" side, replicate(_table) first calls into the "server" side with init_replicate(_table), retrieving replicate_stream_id
--   then constructs a datagram (i.e. connectionless) socket to listen on, and calls into the "server" side 
--   register(replicate_stream_id, listen_addr)
--   which simply inserts that tuple into the table my_pg_replicate_{..??}

-- TODO: look into if it's worth it to use Skype's postgres event queue library instead of rolling our own

-- XXX replicate_stream_id is unclear at the moment; I know I need something like it to distinguish which feed of changes is being fed (at the moment, the only feeds are tables, but in the future we want to support distinct views, in order to be able to do /filtered/ replication).

--  
-- Much of the effort to go out of the way to use the DB itself as storage is because, presumably for security reasons, postgres runs multiple python processes (presumably the same is true for perl)--one per *client connection*. There is no way to just create global python objects and share them across all users. In particular, there's no way to create a pure python queue object for each client to listen on. However, that's what sockets are good for; plus, sockets (or, equivalently, a FIFO or a file) come with blocking built in for free (other options would require polling).
-- This restriction's probably a good thing. It also takes the complication of python-level concurrency out of the picture.


-- BUG: reloading this script doesn't destroy watch_table triggers (though DROP TABLE CASCADE does)
--  possible solutions:
--      use a static naming scheme + CREATE OR REPLACE
--      read my_pg_replicate_clients at load time


select 'installing my_pg_replicate';
DROP TABLE IF EXISTS my_pg_replicate;
CREATE TABLE my_pg_replicate (_table text, event json);

select 'installing my_pg_replicate_broadcast';
CREATE OR REPLACE FUNCTION my_pg_replicate_broadcast() RETURNS trigger AS
$$
  """
  this trigger stops anything actually getting into the table
  """
  import socket
  from warnings import warn
  
  # get "function arguments"
  table, event = TD["new"]["_table"], TD["new"]["event"]
  
  # TODO...
  print("<broadcast>")
  print("[my_pg_replicate_broadcast()]: broadcasting ", event, "on", table)
  
  plant = plpy.prepare("select * from my_pg_replicate_clients where _table = $1", ['text'])
  clients = plpy.execute(plant, [table])
  print("\tto")
  sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM);
  for o in clients:
    print(o)
    try:
      event = event.encode("utf-8")
      
      plpy.debug("\twriting |%s| to %s" % (event, sock.getsockname()))
      sock.sendto(event, o['addr'])
    except Exception as e:
      warn("unable to write to client socket %s: %s" % (o['addr'], e))
      pass
  
  sock.close()
  print("</broadcast>")
  return "SKIP"
$$ LANGUAGE plpython2u;

select 'installing my_pg_replicate_trigger';
CREATE TRIGGER my_pg_replicate_trigger
  BEFORE INSERT  -- We only catch INSERT here, because we mean to
  ON my_pg_replicate -- maintain the invariant that this table is empty
  FOR EACH ROW
  EXECUTE PROCEDURE my_pg_replicate_broadcast();



select 'installing watch_table';
CREATE OR REPLACE FUNCTION watch_table() RETURNS trigger AS $$ 
  """
   This trigger formats events on tables into Hobo Replication Delta JSON (HRDJ)
    and pushes it up to my_pg_replicate
  """
  # triggers do not have arguments passed
  # instead, pl/python passes the trigger data *and* any arguments given when you define the trigger in sub-members of the global variable TD
  
  import json
  import errno
  
  def trigger_to_hrdj(TD):
    "reformat an event in pl/python TD to HRDJ (hobo-replication-delta-json) format"      
    
    # HRDJ Format
    # --------------------------
    # a json object with a single "new" for an insert, a single "old" for a deletion, and both an "old" and a "new" for; beware that this makes the type of change *implicit*; it has the added bonus that it is reallllly simple to consume, since consumers can just look for an "old" tag, act on it, then for a "new" tag and act on that (but have the option of being more clever)
    # TODO: define some kind of remote primary key semantics and, for tables that have SQL primary keys, compress 'old' to only contain the PK, and in the case of an update, compress "new" to only contain the changed fields
    
    if TD["event"] == "INSERT":
      payload = {"+": TD["new"]}
    elif TD["event"] == "UPDATE":
      payload = {"-": TD["old"], "+": TD["new"]}
    elif TD["event"] == "DELETE":
      payload = {"-": TD["old"]}
    payload = json.dumps(payload)
    return payload
  
  print("watch_table() is sending ", TD)
  plant = plpy.prepare("insert into my_pg_replicate values ($1, $2)", ['text','json'])
  plpy.execute(plant, [TD["table_name"], trigger_to_hrdj(TD)])
  
$$ language plpython2u;




-- "-_table" should eventually become "stream_id" with a spare table mapping stream_id to (table, columns, where)
select 'installing my_pg_replicate_clients';
DROP TABLE IF EXISTS my_pg_replicate_clients;
CREATE TABLE my_pg_replicate_clients (stream_id text PRIMARY KEY, addr text UNIQUE, _table text);



select 'installing my_pg_replicate_triggers';
-- a simple data structure to refcount how active clients are watching any table
--  when the number of clients drop to zero, we can remove the parasitic trigger
DROP TABLE IF EXISTS my_pg_replicate_triggers;
CREATE TABLE my_pg_replicate_triggers (_table text PRIMARY KEY, uses int);


select 'installing my_pg_replicate_register';
--TODO: this function is simple enough that it could be done in plpgsql, which should be faster
CREATE OR REPLACE FUNCTION my_pg_replicate_register(_table text, addr text) RETURNS text AS
$$
  # returns the stream id for unregistering (& etc)
  
  import uuid
  stream_id = uuid.uuid4().hex
  
  plant = plpy.prepare("insert into my_pg_replicate_clients values ($1, $2, $3)", ["text", "text", "text"])
  plpy.execute(plant, [stream_id, addr, _table]);
  
  # this function should also install triggers on _table if they are not already there
  # XXX SQL injection
  r = plpy.execute("select _table from my_pg_replicate_triggers where _table = '%s'" % (_table,))
  #import IPython; IPython.embed()
  if r.nrows():
    #XXX SQL injection
    plpy.execute("update my_pg_replicate_triggers set uses = uses + 1 where _table = '%s'" % (_table,))
  else:
    # table is currently unwatched; generate a random name and install the trigger
    # XXX edge case: if we drop the replicate_triggers table while triggers are still active  (eg reloading this file) it's possible that the trigger will already be installed
    #  Since the number of times this happens should be small (famous last words..), it won't hurt to just recreate it every time
    plpy.execute("DROP TRIGGER IF EXISTS watch_table on %s" % (_table,))
    plpy.execute(
    """CREATE TRIGGER watch_table
    AFTER INSERT OR UPDATE OR DELETE
    ON %s
    FOR EACH ROW
    EXECUTE PROCEDURE watch_table()
    """ % (_table))
    
    # record that the trigger is installed, initializing refcount to 1
    plant = plpy.prepare("insert into my_pg_replicate_triggers values ($1, $2)", ['text', 'int'])
    plpy.execute(plant, [_table, 1])
  
  return stream_id; 
$$ LANGUAGE plpython2u;



select 'installing my_pg_replicate_unregister';
CREATE OR REPLACE FUNCTION my_pg_replicate_unregister(stream_id text) RETURNS bool AS
$$
  plant = plpy.prepare("delete from my_pg_replicate_clients where stream_id = $1", ["text"])
  plpy.execute(plant, [stream_id])
  return True; #TODO: check if the query did anything (?? does execute() return a list of affected rows or something we can look at?)
  
  # TODO:
  # this function should also uninstall triggers if there are no listeners for _table left
$$ LANGUAGE plpython2u;
